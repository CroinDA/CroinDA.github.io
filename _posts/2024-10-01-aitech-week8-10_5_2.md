---
layout: single
title: "[ML 경진대회/기법] Retrieval"
categories:
  - ml_technique
tags:
  - 부스트캠프
  - AITech
  - recsys
  - ML
  - Retrieval
  - embedding
  - Arcface
  - CLIP
author_profile: false
use_math: true
---
## 1. Retrieval
### 개요
- 대규모 데이터에서 <mark style="background: #FFF3A3A6;">사용자의 질의(Query)에 가장 적합한 정보(문서, 문장, 이미지 등)</mark>를 찾아내는 기술
	- 고차원 데이터를 벡터 공간에 매핑해 의미적 유사성 계산
	- 단어 간 의미가 유사하면 들에 대한 임베딩 벡터가 벡터 공간 상에서 가깝다는 점을 활용
- 기존 키워드 매칭을 넘어 <mark style="background: #FFF3A3A6;">맥락과 의미를 이해</mark>하는 검색을 가능하도록<br><br>

### 특징
- DB가 충분히 크고 임베딩 방법 및 성능이 우수하다면, 추가 학습 없이 Classification task를 수행 가능함
	- DB에서 검색된 아이템들이 가진 주된 클래스를 활용
- 임베딩 방법에 따라 <mark style="background: #FFF3A3A6;">다른 modality를 가진 데이터 검색</mark>도 가능
	- 텍스트로 이미지 검색<br><br>

### Retrieval 적용 예시: Facial Recognition
- In "Closed Set" → 학습 데이터에 포함된 사람들로만(=제한된 환경에서) 작동
	- 테스트 단계에서, 입력된 얼굴이 반드시 학습 데이터에 등장한 사람 중 하나로 분류됨을 전제
	- ex) 학습 데이터에 A, B, C 세 사람의 얼굴 이미지가 포함된 경우, 모델은 이 세 명의 얼굴만 인식하도록 설계 / 테스트 단계에서 입력된 얼굴은 A,B,C 중 하나로 분류되며 새로운 사람의 얼굴이 입력되면 오류 발생
- 시스템 구성 요소
	- Training Set
		- 여러 개인의 얼굴 이미지들이 DB로 구성됨
		- 각 사람 별 다양한 각도, 조명, 표정의 이미지들이 포함
		- 이러한 데이터들은 <mark style="background: #FFF3A3A6;">임베딩 벡터로 변환</mark>되어 저장
	- Testing Set
		- 시스템이 식별해야 할 새로운 얼굴 이미지
		- <mark style="background: #FFF3A3A6;">실제 시스템이 적용되는 환경</mark>에서 입력되는 데이터(by. 실시간 카메라 입력 or 업로드)
- Facial Recognition 내 Retrieval 과정
	1. 얼굴 탐지 및 전처리
		- 입력된 이미지에 대한 얼굴 영역을 우선 탐지
			- 알고리즘 by Haar Cascade / CNN
			- 변수 조정(조명/각도/배경 등)
			- 얼굴 영역만 추출하여 표준화
	2. 특징 추출
		- 탐지된 얼굴로부터 고유한 특징 추출
			- 전통적 방법 - 기하학적 특징 추출(눈 사이 거리, 코 모양, 턱선 등)
			- 딥러닝 방법 - <mark style="background: #FFF3A3A6;">CNN이 계층적</mark>으로 얼굴 특징 학습 → <mark style="background: #FFF3A3A6;">임베딩 벡터 생성</mark>
			- 결과물 → Faceprint(고유한 수치 벡터)
	3. 데이터베이스 매칭(Retrieval)
		- 생성된 임베딩 벡터를 학습 데이터베이스와 비교
		- Retrieval 활용의 이유
			- <mark style="background: #FFF3A3A6;">모든 개인을 클래스로 두고 학습하는 것은 무리</mark>
			- <mark style="background: #FFF3A3A6;">DB 내 사진과의 거리 비교</mark>를 통해 <mark style="background: #FFF3A3A6;">가까운 거리에 위치한</mark> 사진에 있는 얼굴을 테스트하는 사람의 얼굴이라 판단
		- 사진끼리의 거리 측정은 → "Metric Learning"을 통해
			- Metric Learning: 데이터 간의 유사성을 측정하는 방법을 학습하는 ML 기법
			- 핵심: 유사한 데이터 포인트는 가깝게 / 유사하지 않은 데이터는 멀리
			- 필요성: 이미지/텍스트와 같은 고차원 데이터에서는 전통적 거리 계산이 비효율 → <u>Metric Learning의 딥러닝</u>을 통해 <mark style="background: #FFF3A3A6;">의미 기반 거리 함수</mark>를 자동으로 학습
		- 유사도 계산 방법
			- 코사인 유사도
			- 유클리드 거리
			- 임계값 설정
	4. 신뢰도 평가 및 결과 출력
		- <mark style="background: #FFF3A3A6;">두 얼굴이 동일인일 확률</mark>로 해석, 이를 기반으로 신뢰도 산출
			1. 분포 기반 확률 계산
				- 두 개의 확률 분포 학습: 동일인 분포($p_s$) , 다른인 분포($p_c$)
					- 동일인 분포(같은 사람의 얼굴 간 거리 분포)
					- 다른인 분포(다른 사람의 얼굴 간 거리 분포)
				- 베이즈 규칙 적용
					- $confidence = \frac{p_s(x)}{p_s(x) + p_c(x)}$
			2. 단순 스케일링
				- 코사인 유사도(범위 -1~1)를 신뢰도(범위 0~1)로 단순 선형 변환
				- 데이터 분포를 무시하는 방법이라 선호되지 않음
				- 그렇기 때문에, 확률적 정확도도 낮음<br><br>

### Distance Metrics
- 여러가지 데이터를 임베딩하면 벡터가 생성됨
- 이렇게 생성된 <mark style="background: #FFF3A3A6;">벡터 사이의 유사도를 표현</mark>하기 위해 <mark style="background: #FFF3A3A6;">여러가지 거리 개념</mark>을 사용함
- 주로 아래와 같은 개념을 사용함<br><br>

1. Euclidean Distance
	- L2 거리, 유클리드 거리
	- 두 점 사이의 일반적인 직선 거리를 의미함
	- <mark style="background: #FFF3A3A6;">Retrieval 수행 시 주로 사용되는 Metric</mark>
	- $d(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \cdots + (p_n - q_n)^2}$
2. Manhatten Distance
	- L1 거리, 맨하탄 거리
	- 좌표의 변화량(절대값)의 합
	- <mark style="background: #FFF3A3A6;">학습 시, weight 크기에 대한 정규화로 많이 사용되는 Metric</mark>
	- $d(p,q)=\vert p_1-q_1 \vert+\vert p_2-q_2 \vert+\cdots +\vert p_n - q_n \vert$
3. Cosine Similarity
	- 두 벡터가 이루는 각도의 코사인값
	- 벡터의 크기보다는 방향 측정에 중점
	- <mark style="background: #FFF3A3A6;">유사도를 높이기 위한 학습 타겟</mark>으로 자주 사용됨
	- 코사인 값이기 때문에 -1과 1 사이의 값, <mark style="background: #FFF3A3A6;">1에 가까울 수록 두 벡터는 유사</mark>
	- $d(p,q) = 1-\frac{p \cdot q}{\vert p \vert\vert q \vert}$<br><br>

## 2. 딥러닝을 이용한 Retrieval
### Arcface
- 얼굴인식 분야에서 많이 활용
- Metric Learning의 일종
- Cosine Distance를 활용, 같은 class끼리는 더 가까이 / 다른 class는 더 멀리<br><br>
- 개요
	- logit: DL 분류 모델에서, 활성함수 적용 전(=활성 함수의 입력값으로 사용되는) 원시 출력값
		- Arcface에서 사용되는 logit 개념은 <mark style="background: #FFF3A3A6;">각도 기반으로 재설계</mark>된 값임
	- 각각 클래스 1과 2의 logit을 계산하기 위한 가중치 벡터 $W_1$, $W_2$     
		![image1](../../images/2024-10-01-aitech-week8-10_5_2/image1.png)
	- ex) 아래와 같이 입력된 피처 $x$(아래 그림에서 파란 점)가 있다면
		- 입력피처에 따른 logit값을 비교하면 $logit_1 > logit_2$(각도 크기에 따른 코사인값 대소비교) → $x$는 클래스 1로 분류됨     
		![image2](../../images/2024-10-01-aitech-week8-10_5_2/image2.png)
		- Decision Boundary: 입력 피처 $x$에 대해, $x$가 가중치 벡터 $W_1$ 및 $W_2$와 이루는 각도가 같아지는 경계선
			- 이 경계선을 기점으로, $W_1$과 가까운 쪽은 클래스 1 / $W_2$와 가까운 쪽은 클래스 2로 분류됨     
			![image3](../../images/2024-10-01-aitech-week8-10_5_2/image3.png)
- 학습 방법
	- Decision Boundary 학습
	- Metric Learning: 같은 클래스끼리는 임베딩 거리 가까이, 다른 클래스끼리는 임베딩 거리 멀리
	- <mark style="background: #FFF3A3A6;">Angular margin</mark>
		- 각도 기반의 Arcface에서 Metric Learning의 목표를 수행하기 위한 핵심 IDEA
		- 같은 클래스끼리는 가깝게 만들기 위한 방법론
		- ex) 만일 $x$가 클래스 1이라면, $logit_1$ 계산 시 각도 $\theta_1$에 마진 $m$을 더해줌
			- 마진 $m$을 더해도 아래 식이 성립하여서, x가 클래스 1으로 분류될 수 있도록 해줌
			- $logit_1 = cos(\theta_1+m) > logit_2 = cos(\theta_2)$     
				![image4](../../images/2024-10-01-aitech-week8-10_5_2/image4.png)
		- 특징
			1. Angular margin 원리에 따라 학습하면, $\theta_1$이 작아지는 방향으로 학습됨
				- 이유 → $x$가 기존 Decision boundary에 가까운 곳에 있다면, $m$을 더하는 순간 클래스 2로 구분될 수 있기 때문     
					![image5](../../images/2024-10-01-aitech-week8-10_5_2/image5.png)
				- 따라서, $x$는 최소한 decision boundary로부터 마진 $m$ 이상 떨어진 곳에 위치해야 함
					- 아래 예시같은 경우, 당연히 반대편 클래스 2의 경우에 대해서도 동일     
					![image6](../../images/2024-10-01-aitech-week8-10_5_2/image6.png)
			2. 위 1번과 같은 특징으로, 임베딩들은 최소한 decision boundary에서 $m$만큼 떨어진 곳에 분포함
			3. 그렇기 때문에, 결과적으로 임베딩들은 가중치 벡터(아래 예시에서는 $W_1$, $W_2$)를 중심으로 분포하게 됨     
				![image7](../../images/2024-10-01-aitech-week8-10_5_2/image7.png)
			4. 마진 m이 너무 커질 경우, 학습이 잘 이루어지지 못함
				- (위 2, 3번 특징에 따라) 기존 decision boundary 근처에는 x가 분포하면 안되기에, 결과적으로 임베딩은 가중치 벡터 중심으로 분포
				- 마진이 클수록 같은 클래스 끼리 모이는 힘이 강해지지만, <mark style="background: #FFF3A3A6;">너무 커지면 물리적으로 학습이 불가능한 상태</mark>에 도달하게 됨     
					![image8](../../images/2024-10-01-aitech-week8-10_5_2/image8.png)
		- 학습 결과 비교(Normal Softmax vs Arcface)
			- Normal Softmax(좌): decision boundary까지 classification을 수행한 결과가 넓게 분포
			- Arcface(우, Angular margin): classification을 수행한 결과가 decision boundary에서 <mark style="background: #FFF3A3A6;">멀리</mark> 떨어져 <mark style="background: #FFF3A3A6;">집중</mark>되어 있음      
				![image9](../../images/2024-10-01-aitech-week8-10_5_2/image9.png)<br><br>

### CLIP(Contrastive Language-Image Pre-training)
- <mark style="background: #FFF3A3A6;">이미지-텍스트 간 연관성을 학습</mark>하는 멀티모달 모델 / OpenAI 개발
- 핵심 아이디어
	- 데이터
		- 대규모의 이미지-텍스트 쌍의 데이터를 활용(4억 쌍의 pair data 활용)
		- 이미지-텍스트 데이터를 동일한 임베딩 공간에 매핑하는 것
	- 학습방법
		- <mark style="background: #FFF3A3A6;">Contrastive Learning</mark> → 모 이미지의 임베딩과 그 이미지에 해당하는 텍스트의 임베딩은 가까워지도록 학습함(그렇지 않은 텍스트의 임베딩은 멀어지도록)
		- 이미지-텍스트 쌍으로 이루어진 데이터를 학습시켜,<mark style="background: #FFF3A3A6;">이미지와 그에 맞는 텍스트(=설명)를 서로 연결</mark>할 수 있음
- CLIP의 구조 및 동작 원리
	- 구조: 듀얼 인코더 구조
		- 텍스트 인코더: 문장을 임베딩 벡터로 변환 / 주로 Transformer 구조 활용
		- 이미지 인코더: 이미지를 임베딩 벡터로 변환 / 주로 ViT, CNN 구조 활용
	- 학습 방법: <u>Contrastive Learning</u>
		- 이미지 별로, 해당 이미지(보라색 열)를 설명하는 텍스트(초록색 행)는 임베딩 공간에서 벡터를 가까이 위치시킴 → 해당 "이미지 벡터-텍스트 벡터"의 내적값(파란색)을 크게 해줌
			- 나머지 서로 관련없는 이미지-텍스트 벡터의 내적값은 작게 해줌     
				![image10](../../images/2024-10-01-aitech-week8-10_5_2/image10.png)
		- 각 내적을 logit으로 취급하고, 이미지_1의 클래스는 1이라고 생각하여 CrossEntropy loss를 낮추는 방향으로 최적화함
			- <mark style="background: #FFF3A3A6;">이미지-텍스트 간 대응될 때만 내적 값을 1</mark>이라 하는 것에 목표
		- 이렇게 대형 데이터를 학습시키고 나면, <mark style="background: #FFF3A3A6;">언어 능력을 갖춘 이미지 모델</mark>을 만들 수 있음
			- 학습에서 본 적이 없는 클래스의 이미지도 이미지 분류가 가능해짐
			  (by. 텍스트 설명을 활용)
			- 일반적인 이미지 분류 모델이 분류하는 방법 대신, <mark style="background: #FFF3A3A6;">텍스트를 직접 입력</mark>하여 <u>이미지 임베딩과 내적값이 가장 큰 클래스를 선택</u>하는 알고리즘
			  (텍스트는 전체 문장이 하나의 임베딩 벡터여야 함에 유의)
			- ex) 입력된 강아지 사진의 임베딩 벡터는 "A photo of a dog"라는 문장에 대한 임베딩 벡터와의 내적값이 가장 큼 → 해당 이미지는 dog로 분류     
				![image11](../../images/2024-10-01-aitech-week8-10_5_2/image11.png)
		- 학습 이후, 새로운 이미지(학습에서 본 적이 없는 클래스의 이미지)를 볼 때도 "이 이미지는 무엇인가?"라는 질문에 <mark style="background: #FFF3A3A6;">텍스트 후보들과 비교하여 가장 가까운 답</mark>을 고를 수 있음(Zero-shot)
	- 활용: Open Vocabulary Object Detection(OVD)
		- 사전 정의된 카테고리의 한계를 넘어, <mark style="background: #FFF3A3A6;">학습 단계에서 보지 못한 새로운 객체를 탐지하고 분류</mark>하는 CV 기술
		- 추가 학습 없이, 보다 구체적이고 다양한 설명에 따라 객체를 탐지할 수 있음 → 경진대회에 유용
			- 실제 'Happy Whale' 대회에서 고래/돌고래를 crop하기 위한 전처리로 Open Vocabulary Object Detection 모델(detic)을 사용하였고, 우수한 성능을 보임
		- ex1) 모델에 "the standing person" 이라는 클래스가 직접적으로 학습되지는 않았지만, 텍스트를 이해할 수 있기 때문에 서 있는 사람만 detect함     
			![image12](../../images/2024-10-01-aitech-week8-10_5_2/image12.png)
		- ex2) 마찬가지로, 모델에 "the jumping person" 이라는 클래스를 직접 학습시키지 않았지만, "the jumping person"라는 텍스트를 이해하기에 점프하는 사람만 detect함     
			![image13](../../images/2024-10-01-aitech-week8-10_5_2/image13.png)<br><br>

### DINO (Distillation of Knowledge with No Labels)
- 2021, Meta AI 발표, Self-supervised Learning(자기지도학습) Model
- DINO 핵심 특징
	- ViT와 결합하여 성능 향상, <mark style="background: #FFF3A3A6;">라벨 없는 이미지 데이터</mark>로부터 의미 있는 특징을 학습하는 모델
	1. Self-supervised Model
		- 라벨이 없는 데이터만을 활용하여 모델을 훈련시키는 기법
		- 기존의 지도학습: 라벨을 직접 만들어야 했음 → Self-supervised Learning을 통해 데이터 자체에서 학습 신호를 만들어
		- 기존 비지도학습 적용 모델: BERT / GPT → DINO는 이를 Vision 분야에 성공적으로 적용해낸 사례
	2. ViT 기반
		- DINO의 백본 네트워크: ViT
		- 이미지를 작은 patch로 나누어, 각 patch를 토큰으로 변환하여 Transformer 구조로 처리하는 방법
		- 특히 작은 크기의 패치를 사용하는 것이 중요하다고 강조 → 좋은 특성을 얻어내는 데에 핵심적 역할
- DINO 핵심 구조
	1. Teacher-Student 구조
		- Knowledge Distilation 방식을 채택하나, 기존과 다른 접근 방식 취함
		- Teacher / Student 두 모델 모두 동일한 신경망 구조를 사용
		- Student 모델
			- 실제로 gradient update가 이루어지는 네트워크
		- Teacher 모델
			- Student 모델 가중치의 이동평균(EMA, Exponential Moving Average)으로만 Update
			- 직접 학습하지 않고, Student의 정보를 조금씩 따라가며 안정적 타겟을 제공
		- Student 모델이 Teacher 모델의 출력을 모방하도록 학습
			- Teacher는 점점 부드럽고 안정적인 신호 제공
			- Student는 더 좋은 임베딩을 학습하도록
	2. Multi-crop 증강 기법(Multi-view Augmentation)
		- 다양한 view 생성 → 한 이미지를 다양한 방식으로 자르고 변형하여 여러 이미지 생성
			- <mark style="background: #FFF3A3A6;">Global views</mark>: 원본의 큰 부분에서 2개를 만듦.
			- <mark style="background: #FFF3A3A6;">Local views</mark>: 원본의 작은 부분에서 여러 개를 만듦
		- 입력 분배
			- Teacher: Global views만 입력으로
			- Student: Global / Local 둘 다 입력으로 → Student가 <mark style="background: #FFF3A3A6;">다양한 관점에서 일관된 임베딩을 학습</mark>할 수 있도록(=이미지의 일부만 보더라도 전체와 비슷한 의미를 파악할 수 있도록)
	3. <mark style="background: #FFF3A3A6;">Collapse</mark> 방지 유도
		- Self-supervised learning에서는 "collapse"현상 자주 발생
			- collapse: 모든 입력이 의미없이 똑같은 임베딩으로 수렴하는 현상
		- 해결책 → collapse 없이 다양한 이미지에 대한 풍부한 임베딩을 학습할 수 있도록
			- Centering: Teacher의 출력에서 평균값 빼줌 → 한 쪽 치우침 방지
			- Sharpening: Softmax의 temperature 값을 낮게 설정(날카로운 분포 생성) → Teacher의 출력이 더 뚜렷한(=확신있는) 값을 내도록
- DINO 학습 과정
	1. 데이터 준비와 증강
		- 라벨(정답)이 없는 이미지 데이터만 준비
		- 각 이미지를 다양하게 자르고 변형해 풍부한 views(Global/Local) 생성
	2. 임베딩 생성
		- Student(Global, Local)와 Teacher(Global) 네트워크에 각각 views 입력
		- 각 view에 대해 임베딩 벡터(특징 벡터)를 만듭니다.
	3. 예측과 정렬
		- Student는 모든 views(Global+Local)에 대해 임베딩 생성
		- Teacher는 Global views에 대해서만 임베딩 생성
		- 같은 이미지에서 나온 서로 다른 views(예: 전체와 부분)의 임베딩이 서로 비슷해지도록 Student를 학습시킵니다.
			- 입력 이미지에 두 개의 다른 augmentation 적용
			- 한 이미지는 <mark style="background: #FFF3A3A6;">teacher 모델</mark>에 통과시켜 <mark style="background: #FFF3A3A6;">타겟</mark> 생성
			- 나머지 한 이미지는 <mark style="background: #FFF3A3A6;">student 모델</mark>에 통과시켜서 <mark style="background: #FFF3A3A6;">이미지에 대한 클래스 예측</mark> 생성     
			![image14](../../images/2024-10-01-aitech-week8-10_5_2/image14.png)
	4. 손실 함수(Cross-Entropy Loss)
		- Student의 출력(Softmax 확률 분포)이 Teacher의 출력과 최대한 비슷해지도록 Cross-Entropy Loss 계산
			- Teacher 출력: centering / sharpening을 거쳐 Collapse 방지
	5. Teacher 업데이트(EMA)
		- Student: 손실 함수를 통해 직접 학습(gradient descent)
		- <mark style="background: #FFF3A3A6;">Teacher</mark>: Student의 <mark style="background: #FFF3A3A6;">가중치의 이동평균(EMA)으로만 천천히 업데이트</mark> → Teacher가 Student보다 더 안정적이고 일관된 목표를 제공할 수 있도록
	6. 반복 학습
		- 위 과정을 수백 에폭(epoch) 반복하면, Student는 점점 더 강력한 임베딩을 학습
- DINO 모델의 의의 및 활용도
	- "라벨이 없는 데이터만으로 의미 있는 이미지 임베딩(시각적 특징)을 얻을" 수 있음
	- 이를 달성함으로서, <mark style="background: #FFF3A3A6;">라벨링된 데이터셋 구축에 드는 비용 및 시간을 절감</mark>할 수 있음
	- 다양한 CV 작업에 즉시 활용 가능: image classification / object detection / image segmentation 등
	- DINO를 통해 모델 사전 학습 시, 다양한 하위 작업에 쉽게 적용 가능 → 효율적인 Transfer Learning 가능