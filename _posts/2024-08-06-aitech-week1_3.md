---
layout: single
title: Creating Tensors
categories:
  - pytorch
tags:
  - 부스트캠프
  - AITech
  - AIBasic
  - PyTorch
author_profile: false
---
## 1. Tensor의 생성

### 1.1 - 1.2 특정한 값으로 초기화된 Tensor 생성, 변환

1. 0으로 초기화된 Tensor 생성
    - 1-D Tensor (5) : a = torch.**zeros(5)**
    - 2-D Tensor (2x3) : b = torch.**zeros([2, 3])**
    - 3-D Tensor (3x2x4) : c = torch.**zeros([3, 2, 4])**
    
2. 0으로 초기화되어 있고, **크기와 자료형이 같은** Tensor 생성
    - Like Tensor e : g = torch.**zeros_like(e)**
    
3. 1로 초기화된 Tensor 생성
    - 1-D Tensor (5) : a = torch.**ones(5)**
    - 2-D Tensor (2x3) : b = torch.**ones([2, 3])**
    - 3-D Tensor (3x2x4) : c = torch.**ones([3, 2, 4])**
    
4. 1로 초기화되어 있고, **크기와 자료형이 같은** Tensor 생성
    - Liken Tensor b : h = torch.**ones_like(b)**

### 1.3 난수로 초기화된 Tensor 생성

1. [0, 1] 구간의 연속균등분포 난수 Tensor 생성
    - 1-D : i = torch.**rand(3)**
    - 2-D : j = torch.**rand([2, 3])**
    
2. 표준정규분포에서 추출한 난수 Tensor 생성
    - 1-D : k = torch.**randn(3)**
    - 2-D : I = torch.**randn([2, 3])**
        
        ⇒ dtype을 **torch.int**로 지정하면 오류 발생
        
3. 연속균등분포에서 추출한 난수로 생성하고, **크기와 자료형이 같은** Tensor 생성
    - Like Tensor k : m = torch.**rand_like(k)**

1. 표준정규분포에서 추출한 난수로 생성하고, **크기와 자료형이 같은** Tensor 생성
    - Like Tensor i : n = torch.**randn_like(i)**

### 1.4 지정된 범위 내에서 초기화된 Tensor 생성

1. 간격이 int (int64)
    - o = torch.**arange(start = 1, end = 11, step = 2)**
2. 간격이 float (float)
    - o = torch.**arange(start = 1, end = 4, step = 0.5)**

### 1.5 초기화되지 않은 Tensor 생성

- “초기화 되지 않았다.”
    
    → 생성된 Tensor의 각 요소가 **명시적으로 다른 특정 값으로 설정되지 않았음**을 의미
    
    → 초기화되지 않은 Tensor 생성 시, 해당 Tensor는 **메모리에 이미 존재하는 임의의 값들로 채워짐**
    
- 초기화 되지 않은 Tensor를 사용하는 이유
    
    1) 성능 향상 : Tensor 생성 직후 **곧바로 다른 값으로 덮어쓸 예정인 경우 → 불필요한 자원 소모 감소**
    
    2) 메모리 사용 최적화 : (큰 Tensor 다룰 때) 불필요한 초기화는 곧 불필요한 자원(메모리 사용량) 소모
    

1. 초기화 되지 않은 Tensor 생성
    - 1-D : q = torch.empty(5)
2. 초기화 되지 않은 Tensor를 **다른 데이터로 수정**
    - 1-D : q.fill_(3.0) ⇒ ‘in-place 연산’ : 기존 Tensor 수정, (당연히) 메모리 주소 유지
        
                 q.fill(3.0) ⇒ ‘일반 연산’ : 새 Tensor 생성, 메모리 주소 바뀜
        

### 1.6 list, Numpy 데이터로부터 Tensor 생성

- List vs Numpy
    1. List s를 Tensor t로 생성 
        
        ⇒  t = torch.**tensor(s)**
        
    2. 2-D Matrix Numpy u에서 Tensor v로 생성
        
        ⇒  v = torch.**from_Numpy(u).float()**
        

### 1.7 CPU Tensor 생성

1. 정수형 CPU Tensor 생성
    
    ⇒ w = torch.IntTensor([1, 2, 3, 4, 5])
    
2. 실수형 CPU Tensor 생성
    
    ⇒ x = torch.FloatTensor([1, 2, 3, 4, 5]) 
    
3. 기타 정수, 실수형 CPU Tensor 생성
    
    1) 8비트 부호 없는 정수형: torch.**Byte**Tensor
    
    2) 8비트 부호 있는 정수형: torch.**Char**Tensor
    
    3) 16비트 부호 있는 정수형: torch.**Short**Tensor
    
    4) 64비트 부호 있는 정수형: torch.**Long**Tensor
    
    5) 64비트 부호 있는 실수형: torch.**Double**Tensor
    

### 1.8 Tensor의 복제

- 1-D Tensor x
    1. y = x.**clone()**
    2. z = x.**detach()**  → clone 메서드와 차이: **새로운 Tensor** z에 저장

### 1.9 CUDA Tensor 생성과 변환

- GPU?
    
    → 그래픽 처리 장치, **AI에서는 대규모 데이터 처리와 복잡한 계산**을 위해 사용
    
    → GPU의 수천개의 코어가 병렬처리작업을 도와 능력 향상
    
1. Tensor가 위치한 현재 디바이스 확인
    
    → **a.device**
    
2. CUDA 기술 사용 가능 환경인지 확인
    
    → torch.**cuda.is_available()**
    
3. CUDA device 이름 확인
    
    → torch.cuda.**get_device_name**(device=0)
    
4. Tensor를 GPU에 할당
    
    → b = torch.tensor([1, 2, 3, 4, 5])**.to(’cuda’) / .cuda()**
    
5. 사용 가능한 GPU 개수 확인
    
    → torch.cuda.**device_count()**
    
6. GPU 할당 Tensor → CPU 할당 Tensor 변환
    
    → c = b**.to(device = ‘cpu’)**
    
    → c = b**.cpu()**