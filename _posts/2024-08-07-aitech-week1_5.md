---
layout: single
title: Basic Operations on Tensors
categories:
  - pytorch
tags:
  - 부스트캠프
  - AITech
  - AIBasic
  - PyTorch
author_profile: false
use_math: true
---
## 1. Tensor의 모양변경 2
### 1.1 cat 함수를 활용한 Tensor들 간의 연결
- cat vs stack
    - stack : 새로운 차원을 생성해서 Tensor 결합
    - cat : <mark style="background: #FFF3A3A6;">기존의 차원을 유지</mark>하면서 Tensor 연결
- cat 메서드
    - `b = torch.tensor([[0, 1], [2, 3]])`
    - `c = torch.tensor([[4, 5]])`
        1. dim = 0 기준으로 연결
	        - d = torch.<mark style="background: #FFF3A3A6;">cat</mark>((b, c)) (tuple로 묶어 입력)
            
        2. dim = 1 기준으로 연결
            - d = torch.cat((b, c), 1) → Error(dim=1 차원의 숫자 안맞음)
		        →  d = torch.cat((b, c.reshape(2,1),1)
            

### 1.2 expand() 메서드를 활용한 Tensor 크기 확장
- Tensor의 **차원의 크기 1**일때 해당 차원의 크기 확장
- `f = torch.tensor([[1, 2, 3]])`
    - f의 크기를 (1, 3)에서 (4, 3)으로 확장
        - `g = f.expand(4, 3)`
        ![image1.png](../../images/2024-08-07-aitech-week1_5/image1.png)
        

### 1.3 repeat()메서드를 활용한 Tensor 크기 확장
- `h = torch.tensor([[1, 2], [3, 4]])`
    - h를 dim = 0축 방향으로 2번 반복, dim = 1 축 방향으로 3번 반복
        - `i = h.repeat(2, 3)`
        ![image1.png](../../images/2024-08-07-aitech-week1_5/image2.png)


## 2. Tensor의 기초 연산
### 2.1 Tensor의 산술연산 - 함수 vs in-place
1. 더하기 연산
- 크기가 같은 Tensor : 각 요소들을 더한 값으로 출력
    - `a = torch.tensor([[1, 2], [3, 4]])`
    - `b = torch.tensor([[1, 3], [5, 7]])`
	    - 일반 연산 → <mark style="background: #FFF3A3A6;">torch.add(a,b)</mark>
	    - in-place 연산 → <mark style="background: #FFF3A3A6;">a.add_(b)</mark> 
			- id 메서드로 고유 식별자 확인 → 메모리 주소 같은지 확인 가능
		
- 크기가 다른 Tensor : **Broadcating** 발생 후 연산
	- `c = torch.tensor([[1, 2], [4, 5]])`
    - `d = torch.tensor([[1, 3]])`
	    → `d = torch.tensor ([[1, 3], [1, 3]])`으로 Broadcasting 발생
	    


2. 빼기 연산
- 크기가 같은 Tensor : 각 요소들을 더한 값으로 출력
    - `e = torch.tensor([[2, 3], [4, 5]])`
    - `f = torch.tensor([[2, 4], [6, 8]])`
	    - 일반 연산 → <mark style="background: #FFF3A3A6;">torch.sub(f, e)</mark> (f-e)
	    - in-place 연산 → <mark style="background: #FFF3A3A6;">f.sub_(e)</mark> 
		    - id 메서드로 고유 식별자 확인 → 메모리 주소 같은지 확인 가능
	    
- 크기가 다른 Tensor : **Broadcasting** 발생 후 연산
    - `g = torch.tensor([[2, 3], [5, 6]])`
    - `h = torch.tensor([[1], [4]])`
		→ `h = torch.tensor ([[1, 1], [4, 4]])`으로 Broadcasting 발생
	    

3. 스칼라곱 연산
- `i = 2`
- `j = torch.tensor([[1, 2], [3, 4]])`
    → <mark style="background: #FFF3A3A6;">torch.mul(i, j)</mark>

4. 요소별 곱하기 연산
- `k = torch.tensor([[1, 2], [1, 3]])`
- `I = torch.tensor([[2, 3], [1, 4]])`
    - 일반 연산 → <mark style="background: #FFF3A3A6;">torch.mul(k, I)</mark>
    - in-place 연산 → <mark style="background: #FFF3A3A6;">k.mul_(I)</mark>
	    - 크기가 다른 Tensor ⇒ Broadcating 발생 후 연산

5. 요소별 나누기 연산
- `o = torch.tensor([[18, 9], [10, 4]])`
- `p = torch.tensor([[6, 3], [5, 2]])`
    - 일반 연산 → <mark style="background: #FFF3A3A6;">torch.div(o, p)</mark>
    - in-place 연산 → <mark style="background: #FFF3A3A6;">o.div_(p)</mark>
	    - 크기가 다른 Tensor ⇒ Broadcating 발생 후 연산

6. 스칼라 거듭제곱(근) 연산
- `s = torch.tensor([[1, 2], [3, 4]])`
- `n = scalar`
    - Tensor s의 각 요소의 n제곱 → <mark style="background: #FFF3A3A6;">torch.pow(s, n)</mark>
    - Tensor s의 각 요소의 n제곱근(1/n승) → <mark style="background: #FFF3A3A6;">torch.pow(s, 1/n)</mark>

7. 요소별 거듭제곱 연산
- `t  = torch.tensor([[5, 4], [3, 2]])`
- `u = torch.tensor([[1, 2], [3, 4]])`
    - 일반 연산 → <mark style="background: #FFF3A3A6;">torch.pow(t, u)</mark>
    - in-place 연산 → <mark style="background: #FFF3A3A6;">t.pow_(u)</mark>

### 2.2 Tensor의 비교연산
- `v = torch.tensor([1, 3, 5, 7])`
- `w = torch.tensor([2, 3, 5, 7])`
    1. 대응요소들이 같은지 비교
	    - torch.<mark style="background: #FFF3A3A6;">eq</mark>(v, w)
        
    2. 대응요소들이 다른지 비교
	    - torch.<mark style="background: #FFF3A3A6;">ne</mark>(v, w)
        
    3. v의 각 요소들이 w의 각 요소들보다 **큰지** 비교 (greater than)
	    - torch.<mark style="background: #FFF3A3A6;">gt</mark>(v, w)
        
    4. v의 각 요소들이 w의 각 요소들보다 **크거나 같은**지 비교 (greater equal)
	    - torch.<mark style="background: #FFF3A3A6;">ge</mark>(v, w)
        
    5. v의 각 요소들이 w의 각 요소들보다 **작은지** 비교 (less than)
        - torch.<mark style="background: #FFF3A3A6;">lt</mark>(v, w)
        
    6. v의 각 요소들이 w의 각 요소들보다 작**거나 같은**지 비교 (less equal)
        - torch.<mark style="background: #FFF3A3A6;">le</mark>(v, w)

### 2.3 Tensor의 논리연산
- 전자 회로를 구성하는 가장 기본적인 논리 게이트
    1. 논리곱 (AND)
        - <mark style="background: #FFF3A3A6;">입력된 신호가 모두 참일때,</mark> 출력이 참이 되는 연산
	    - torch.<mark style="background: #FFF3A3A6;">logical_and</mark>(x, y)
	        - 참 + 참 → 참
	        - 참 + 거짓 → 거짓
	        - 거짓 + 참 → 거짓
	        - 거짓 + 거짓 → 거짓
    2. 논리합 (OR)
        - <mark style="background: #FFF3A3A6;">입력된 신호가 둘 중 하나라도 참일때,</mark> 출력이 참이 되는 연산
        - torch.<mark style="background: #FFF3A3A6;">logical_or</mark>(x, y)
	        - 참 + 참 → 참
	        - 참 + 거짓 → 참
	        - 거짓 + 참 → 참
	        - 거짓 + 거짓 → 거짓
    3. 배타적 논리합 (XOR)
        - <mark style="background: #FFF3A3A6;">입력된 신호가 둘 중 하나만 참일때,</mark> 출력이 참이 되는 연산
	    - torch.<mark style="background: #FFF3A3A6;">logical_xor</mark>(x, y)