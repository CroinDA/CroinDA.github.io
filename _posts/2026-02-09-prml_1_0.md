---
layout: single
title: "[PRML / 패턴인식과 머신러닝 / Chapter 1] 패턴인식 및 학습 알고리즘 소개"
date: 2026-02-09
categories:
  - prml_major_chapter1
tags:
  - PRML
  - 패턴인식
  - 머신러닝
  - 인공지능
  - 지도학습
  - 비지도학습
  - 강화학습
  - 분류
  - 클러스터링
  - 손글씨 인식
  - MNIST
  - 특징 추출
  - 전처리
  - 차원감소
author_profile: false
use_math: true
---

# 개요

## 패턴인식

- 주어진 데이터에서 패턴을 찾아내는 일
- 때로는 아주 중요한 문제임
    - ex1) 요하네스 케플러: 티코 브라헤가 16세기에 관찰 및 축적해둔 대량의 천문학 데이터→ 패턴 발견 → 고전역학의 밑거름이 됨
    - ex2) 원자 스펙트럼에서 규칙성 발견 → 20세기 초, 양자물리학의  발전 및 확인에 중대한 역할

- 패턴인식 과정 정리
    1. 컴퓨터 알고리즘 활용
    1. 데이터의 규칙성을 찾아줌(자동화)
    1. 이 규칙성을 이용해 여러 가지 일을 하는 분야(데이터를 각 카테고리로 분류 등)


## 유명한 데이터셋 예시: 손글씨 숫자 데이터

### 데이터셋 소개

- 각 숫자는 28*28 픽셀의 이미지 → 784개의 실수로 구성된 벡터로 표현
- 목표: 벡터 x를 입력으로 받을 때 → 숫자 0~9 중 하나의 값을 올바르게 출력하는 기계 생성
- 도전: 사람마다 손글씨의 모양은 제각각임 → 규칙적이지 않으므로, 그리 쉬운 문제만은 아님

### 문제 해결책


![](/images/2026-02-09-prml_1_0/image1.png)

- 직접 작성 / 휴리스틱 알고리즘
    - 완벽한 최적해 대신 **복잡한 문제에 대해 제한된 시간 내**에 현실적으로 '충분히 좋은' 근사해를 찾는 방식
    - 경험 기반의 문제 해결 방식
    - 계산 비용이 너무 높거나 정확한 알고리즘이 없는 경우 → 시행착오, 직관, 추론 등을 통해 탐색 공간을 줄여 빠르게 해답을 도출
    - 하지만, 이를 통해 해결하려고 하면 **수많은 규칙이 필요**해짐. 또한 **각각의 규칙에 대한 예외사항** 적용, 예외사항에 또 예외사항 적용해야 됨 → **끊임없는 룰 생성**이 반복됨, 이와 함께 예외없이 **늘상 좋지 못한 성능 **도출됨

- 머신러닝
    - 휴리스틱 알고리즘보다 훨씬 더 좋은 결과물을 얻을 수 있음
    - 해결책 상세 → “N개의 숫자들을 **훈련 집합으로 활용**하여 변경 가능한 **모델의 매개변수 조절**”
        - 훈련 집합에 있는 숫자들의 카테고리(=정답) → “미리 주어짐”
            - 일반적으로, 사람이 각각을 직접 검사해 **수동으로 카테고리를 부여**함
            - 그 카테고리(= 각 숫자의 카테고리) → “표적벡터(target vector) t”

        - 표적벡터 t → **해당 숫자의 정체가 실제로 무엇인지** 나타내줌
        - 각 **숫자 이미지 x에 대한 표적벡터 t는 오직 하나**


- **특징 추출(Feature Extraction)**
    - **입력 변수들을 전처리**하여 새로운 변수 공간으로 전환 → 패턴 인식 문제를 더 쉽게 해결 가능
    - ex) 손글씨 숫자 데이터셋
        - 숫자 이미지들을 → 각 숫자가 **고정된 크기의 박스에 들어가도록** 변환/축소/확대
        - 이는, 각 숫자 클래스 내에서의 가변성을 상당히 줄여줌 → 각 숫자들의 **위치 및 척도가 같기 때문**
        - 이러한 특성은, 패턴인식 알고리즘이 각 클래스를 구별해내기 더욱 용이하게 만들어줌
        - 이러한 **전처리 과정을 특징 추출**이라고 부름
        - 또한 이러한 전처리 과정은, training data 뿐 아니라 **test data에도 동일하게 적용**해 주어야 함


- **차원 감소(Dimensionality Reduction)**
    - **계산 속도를 높이기** 위한 전처리 과정 중 하나
    - ex) 고해상도의 비디오 스트림에서 실시간 얼굴 인식
        - 컴퓨터는 초마다 많은 픽셀을 다루어야 함 → 이 수많은 픽셀 데이터들을 복잡한 패턴 인식 알고리즘에 적용하기는 계산적으로 어려움
        - 대신, 얼굴과 얼굴이 아닌 것을 **차별적으로 구별하는 정보 + 빠르게 계산하는 것이 가능**한 유용한 특징을 찾아내 사용
        - 이라한 특징들(차별적으로 구별하는 정보 및 빠르게 계산이 가능한 유용한 특징 등)을 **패턴 인식 알고리즘의 입력값**으로 활용 → “효과적으로 얼굴인식 문제 해결 가능”
            - 주어진 모 이미지에서 **사각형 모양 소구역의 평균 영상 강도**를 계산한다고 할 때, 매우 효율적인 연산 가능
            - 그리고 그러한 특징들은 얼굴 인식 문제에 있어서 매우 효율적임이 증명됨(Viola and Jones, 2004)

        - 이러한 종류의 전처리를 **차원 감소**라고 부름



<br>

## 학습 알고리즘의 종류

### 지도 학습(Supervised Learning)

- 지도 학습 문제: 주어진 훈련데이터가 **“입력벡터 + 그에 해당하는 표적 벡터”**로 이루어진 문제
- 지도 학습 문제의 종류
    - 분류(Classification)
        - (앞선 숫자 인식 문제로 보면) 각각의 입력 벡터를 제한된 숫자의 분리된 카테고리 중 하나에 할당하는 종류의 지도학습 문제

    - 회귀(Regression)
        - 기대되는 출력값이 하나 또는 그 이상의 연속된 값일 경우
        - ex) 반응 물질의 농도, 온도, 압력이 주어질 경우 → 화학 반응을 통해 결과물이 얼마나 산출될지 예측하는 문제



### 비지도 학습(Unsupervised Learning)

- 훈련 데이터가 **표적 벡터 없이 오직 입력 벡터 x**만 주어지는 경우
- 비지도 학습 문제의 종류(예시)
    - Clustering
        - 데이터 내에서 **비슷한 예시들의 집단**을 찾는 문제

    - Density estimation
        - 입력 공간에서의 **데이터의 분포**를 찾는 문제

    - Visualization
        - 고차원의 데이터를 상대적으로 낮은 차원(이차원, 삼차원)으로 투영해 이해하기 쉽게 만들어 보여주는 기법



### 강화학습(Reinforcement Learning)

- 주어진 상황에서, **보상을 최대화하기 위한 행동**을 찾는 문제를 푸는 방법
- 강화학습은 **시행착오를 통해 입력값 및 최적의 출력값을 직접 찾아냄**
    - 지도학습과 달리, 학습 알고리즘의 입력값 및 최적의 출력값을 예시로 주지 않음
    - 보통의 경우, 알고리즘은 **주변 환경과 상호 작용 시 일어나는 일들을 표현**한 “일련의 연속된 상태 및 행동들”이 문제로 주어짐
    - 현재의 행동은 바로 직후의 보상 뿐 아니라, **다음 시간 단계들 전부의 보상에 영향**을 미침

- 강화학습 예시: 백개먼(backgammon, 서구권의 주사위놀이/윷놀이와 흡사)
    - 개요
        - 뉴럴 네트워크에 적절한 강화 학습 테크닉 활용 시, 고수준의 실력을 갖춘 알고리즘을 훈련시킬 수 있음
        - 입력 ex: 현재 보드 상태 및 주사위 결과 → 출력 ex: 강해진(=이길 수 있는) 다음 플레이
        - 오랜 시간 ML의 주요 연구 분야 중 하나

    - 학습 과정
        - 해당 네트 워크 알고리즘을 자신의 복사본과 **수백만 번 이상** 개임을 수행

    - 어려운 점
        - 플레이어는 백개먼 게임을 수행하면서, 수십가지 선택을 할 수 있음
        - 하지만, 보상은 오직 **게임이 끝났을 때 승리라는 형태**로만 주어짐
        - 그 보상은 **최종 승리까지 이끄는 모든 선택지**에 대해 잘 “분배”되어야 함


- **신뢰 할당(Credit Distribution)**
    - 강화학습을 수행하면서, 보상을 수령하는 과정은 **최종 승리까지 이끄는 모든 선택지**까지 적절히 분배되어야 함
    - 이 “보상을 분배하는” 과정은, 어떤 것은 좋은 선택지이고 또 어떤 것은 덜 좋은 선택지일 지라도 적절히 분배되어야 함

- “Exploration-Exploitation” Tradeoff
    - 강화학습에는 탐사와 이용 간 트레이드오프가 있음
        - 탐사(Exploration) 과정: 시스템이 새로운 종류의 행동 시도 → 각각 얼마나 효과적인지 확인
        - 이용(Exploitation) 과정: 시스템이 높은 보상을 주는 것으로 알려진 행동 시행

    - **둘 중 하나에 너무 집중한 알고리즘**은 썩 좋지 못한 결과를 불러일으킴


## 요약

- 위와 같은 다양한 알고리즘 해결에는
    - 각 알고리즘은 서로 다른 방법과 기술이 필요함
    - 이와 동시에, **핵심에 속하는 중요 아이디어는 겹침**

- 앞으로의 목표
    - 핵심에 속하는, 겹치는 중요 아이디어에 대해 다양한 예시로 쉽게 설명
    - 앞으로 내용에서 필요한 중요한 도구: “확률론+의사 결정 이론+정보 이론”


<br>

